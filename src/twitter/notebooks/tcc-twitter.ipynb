{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mineração de *Tweets*\n",
    "Dados coletados durante o domingo (17/04/2015) de votação do Congresso para a continuação do processo de Impeachment da senhora Presidente Dilma Rousseff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib import rcParams\n",
    "from mpltools import style\n",
    "from matplotlib import dates\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import random\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('mac_morpho')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import operator \n",
    "from collections import Counter\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk import bigrams \n",
    "\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "from pandas.tseries.resample import TimeGrouper\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "\n",
    "from pandas.tseries.resample import TimeGrouper\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "import time\n",
    "\n",
    "import pytz\n",
    "from pandas.tseries.resample import TimeGrouper\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "# from textblob import TextBlob\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib\n",
    "# import seaborn as sns\n",
    "# import cartopy\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# pd.options.display.mpl_style = 'default'\n",
    "# matplotlib.style.use('ggplot')\n",
    "# sns.set_context('talk')\n",
    "# sns.set_style('whitegrid')\n",
    "# % matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"deep\", desat=.6)\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 4)})\n",
    "style.use('ggplot')\n",
    "rcParams['axes.labelsize'] = 9\n",
    "rcParams['xtick.labelsize'] = 9\n",
    "rcParams['ytick.labelsize'] = 9\n",
    "rcParams['legend.fontsize'] = 7\n",
    "rcParams['font.serif'] = ['Computer Modern Roman']\n",
    "rcParams['text.usetex'] = False\n",
    "rcParams['figure.figsize'] = 20, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_data_path = 'data-docker/tmp/dilmahash.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print len(tweets_data)\n",
    "print tweets_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes de *token* e plots com *vicent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = \"RT @medeirosthiiago: testando exemplo TCC! :D http://example.com #ImpeachmentDay\"\n",
    "print preprocess(tweet)\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        count_all.update(terms_all)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "# stop = stopwords.words('english') + punctuation + ['RT', 'o']\n",
    "stop = nltk.corpus.stopwords.words('portuguese') + punctuation + ['RT', 'n']\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        count_all.update(terms_stop)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_single = set(terms_all)\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) \n",
    "                      if term.startswith('#')]\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                      if term not in stop and\n",
    "                      not term.startswith(('#', '@'))] \n",
    "        count_all.update(terms_hash)\n",
    "    except:\n",
    "        continue        \n",
    "\n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_bigram = bigrams(terms_stop)\n",
    "        count_all.update(terms_bigram)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "\n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                      if term not in stop \n",
    "                      and not term.startswith(('#', '@'))]\n",
    "        for i in range(len(terms_only)-1):            \n",
    "            for j in range(i+1, len(terms_only)):\n",
    "                w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "                if w1 != w2:\n",
    "                    com[w1][w2] += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "com_max = []\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])\n",
    "\n",
    "search_word = sys.argv[1]\n",
    "count_search = Counter()\n",
    "print 'ok'\n",
    "\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    "        if search_word in terms_only:\n",
    "            count_search.update(terms_only)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "            \n",
    "print \"Co-occurrence for %s:\" % (search_word)\n",
    "print count_search.most_common(20)\n",
    "\n",
    "\n",
    "\n",
    "import vincent\n",
    " \n",
    "word_freq = count_terms_only.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_hash = [term for term in preprocess(tweet['text'])]\n",
    "        # track when the hashtag is mentioned\n",
    "        if '#ImpeachmentDay' in terms_hash:\n",
    "            date.append(tweet['created_at'])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "ones = [1]*len(date)\n",
    "idx = pandas.DatetimeIndex(date)\n",
    "date_tweet = pandas.Series(ones, index=idx)\n",
    " \n",
    "\n",
    "per_minute = date_tweet.resample('1H', how='count')\n",
    "\n",
    "vincent.core.initialize_notebook()\n",
    "\n",
    "time_chart = vincent.Line(date_tweet)\n",
    "time_chart.axis_titles(x='Tempo', y='Frequencia')\n",
    "time_chart.colors(brew='Spectral')\n",
    "time_chart.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_data = dict(ForaDilma=per_minute_i, NaoVaiTerGolpe=per_minute_s, ForaCunha=per_minute_e)\n",
    "\n",
    "\n",
    "all_matches = pandas.DataFrame(data=match_data,\n",
    "                               index=per_minute_i.index)\n",
    "\n",
    "all_matches = all_matches.resample('30S', how='sum').fillna(0)\n",
    " \n",
    "\n",
    "vincent.core.initialize_notebook()\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart = vincent.Line(date_tweet)\n",
    "time_chart.axis_titles(x='Tempo', y='Frequencia')\n",
    "time_chart.colors(brew='Spectral')\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testes com a ferramenta *arrow* e horários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "tweets['created_at'] = map(lambda tweet: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')), tweets_data)\n",
    "tweets['user'] = map(lambda tweet: tweet['user']['screen_name'], tweets_data)\n",
    "tweets['user_followers_count'] = map(lambda tweet: tweet['user']['followers_count'], tweets_data)\n",
    "tweets['retweet_count'] = map(lambda tweet: tweet['retweet_count'], tweets_data)\n",
    "tweets['favorite_count'] = map(lambda tweet: tweet['favorite_count'], tweets_data)\n",
    "\n",
    "tweets['text'] = map(lambda tweet: tweet['text'].encode('utf-8'), tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "tweets['Location'] = map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data)\n",
    "\n",
    "tweets.info()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_time = pd.Series()\n",
    "\n",
    "tweets_time['created_at'] = map(lambda tweet: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')), tweets_data)\n",
    "tweets_time['created_at'] = pd.to_datetime(pd.Series(tweets_time['created_at']))\n",
    "tweets_time.se('created_at', drop=False, inplace=True)\n",
    "tweets_time.index = tweets_time.index.tzlocalize('GMT').tzconvert('EST')\n",
    "tweets_time.index = tweets_time.index - DateOffset(hours = 12)\n",
    "tweets_time.index\n",
    "\n",
    "tweets_time.head()\n",
    "tweets_time.mean()\n",
    "print 'testou'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = pytz.timezone('Europe/Warsaw')\n",
    "tweets.set_index('created_at', drop=False, inplace=True)\n",
    "tweets.index = tweets.index\n",
    "tweets.index = tweets.tz_localize('GMT').tz_convert('EST')\n",
    "tweets.index = tweets.index - DateOffset(hours = 12)\n",
    "\n",
    "tweets_time = tweets['created_at']\n",
    "tweets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets['created_at'].value_counts())\n",
    "df['hour'] = df.index\n",
    "\n",
    "hours = [item.split(\" \")[0] for item in df['hour'].values]\n",
    "df['hours'] = hours\n",
    "grouped_tweets = df[['hours']].groupby('hours')\n",
    "tweet_growth = grouped_tweets.sum()\n",
    "tweet_growth['hours']= tweet_growth.index\n",
    "\n",
    "tweet_growth\n",
    "# df.info()\n",
    "# df.tail()\n",
    "# df.mean()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "import vincent\n",
    "vincent.core.initialize_notebook()\n",
    "area = vincent.Area(df)\n",
    "area.colors(brew='Spectral')\n",
    "area.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print stopwordsnltk.corpus.mac_morpho.words()\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "text = tweets['text']\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for txt in text.values:\n",
    "    tokens.extend([t.lower().strip(\":,.\") for t in txt.split()])\n",
    "\n",
    "\n",
    "\n",
    "filtered_tokens = [w for w in tokens if not w in stop]\n",
    "\n",
    "freqdist = nltk.FreqDist(filtered_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist.keys()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flyers.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_original_tweets = [element for element in tweets['text'].values if not element.startswith('RT')]\n",
    "print list_of_original_tweets[0]\n",
    "\n",
    "print \"Number of Original Tweets : \" + str(len(list_of_original_tweets))\n",
    "\n",
    "list_of_retweets = [element for element in tweets['text'].values if element.startswith('RT')]\n",
    "print \"Number of Retweets : \" + str(len(list_of_retweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_tweets_per_category(category, title, x_title, y_title, top_n=5, output_filename=\"plot.png\"):\n",
    "    \"\"\"\n",
    "    :param category: Category plotted, can be tweets users, tweets language, tweets country etc ..\n",
    "    :param title: Title of the plot\n",
    "    :param x_title: List of the items in x\n",
    "    :param y_title: Title of the variable plotted\n",
    "    :return: a plot that we can save as pdf or png instead of displaying to the screen\n",
    "    \"\"\"\n",
    "    tweets_by_cat = category.value_counts()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.tick_params(axis='x')\n",
    "    ax.tick_params(axis='y')\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "    tweets_by_cat[:top_n].plot(ax=ax, kind='bar')\n",
    "    fig.savefig(output_filename)\n",
    "    fig.show()\n",
    "\n",
    "plot_tweets_per_category(tweets['lang'], \"#ImpeachmentDay by Language\", \n",
    "                         \"Language\", \n",
    "                         \"Number of Tweets\", \n",
    "                         2000,\n",
    "                         \"mozsprint_per_language.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_tweets_per_category(tweets['Location'], \n",
    "                             \"#ImpeachmentDay by Location\", \n",
    "                             \"Location\", \n",
    "                             \"Number of Tweets\", 2000,\n",
    "                             \"ImpeachmentDay_per_location.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_tweets_per_category(tweets['user'], \n",
    "                             \"#ImpeachmentDay active users\", \n",
    "                             \"Users\", \n",
    "                             \"Number of Tweets\", 20,\n",
    "                             \"ImpeachmentDay_users.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(category, title, x_title, y_title, output_filename=\"plot.png\"):\n",
    "        \"\"\"\n",
    "        :param category: Category plotted, can be users, language, country etc ..\n",
    "        :param title: Title of the plot\n",
    "        :param x_title: List of the items in x\n",
    "        :param y_title: Title of the variable plotted\n",
    "        :return: a plot that we can save as pdf or png instead of displaying to the screen\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.tick_params(axis='x')\n",
    "        ax.tick_params(axis='y')\n",
    "        ax.set_xlabel(x_title)\n",
    "        ax.set_ylabel(y_title)\n",
    "        ax.set_title(title)\n",
    "        sns.distplot(category.values, rug=True, hist=True);\n",
    "        fig.savefig(output_filename)\n",
    "\n",
    "\n",
    "plot_distribution(tweets['retweet_count'], \n",
    "                      \"#ImpeachmentDay retweets distribution\", \"\", \"\",\n",
    "                      \"retweets_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets['created_at'].value_counts(), columns=['number_tweets'])\n",
    "df['date'] = df.index\n",
    "df.head()\n",
    "\n",
    "days = [item.split(\" \")[0] for item in df['date'].values]\n",
    "df['days'] = days\n",
    "grouped_tweets = df[['days', 'number_tweets']].groupby('days')\n",
    "tweet_growth = grouped_tweets.sum()\n",
    "tweet_growth['days']= tweet_growth.index\n",
    "\n",
    "tweet_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(len(tweet_growth['days'].values))\n",
    "ax.bar(x_pos, tweet_growth['number_tweets'].values, align='center')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_title('#ImpeachmentDay hashtag growth')\n",
    "ax.set_ylabel(\"number tweets\")\n",
    "ax.set_xticklabels(tweet_growth['days'].values)\n",
    "fig.savefig('ImpeachmentDay.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join(tweets['text'].values.astype(str))\n",
    "\n",
    "no_urls_no_tags = \" \".join([word for word in text.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=2000,\n",
    "                      stopwords=STOPWORDS, width=1800, height=1400).generate(no_urls_no_tags)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "\n",
    "text = \" \".join(tweets['text'].values.astype(str))\n",
    "\n",
    "no_urls_no_tags = \" \".join([word for word in text.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "tweet_coloring = imread(path.join(\"dilma2.png\"))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=tweet_coloring,\n",
    "               stopwords=STOPWORDS, max_font_size=40, random_state=42)\n",
    "\n",
    "wc.generate(no_urls_no_tags)\n",
    "\n",
    "image_colors = ImageColorGenerator(tweet_coloring)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.savefig('mozsprint.png', dpi=300)\n",
    "plt.imshow(wc.recolor(color_func=image_colors))\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(tweet_coloring, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dados minerados anteriormente - OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "tweets['country'] = map(lambda tweet: tweet['place']['country']\n",
    "                        if tweet['place'] != None else None, tweets_data)\n",
    "\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_xlabel('Línguas'.decode('utf-8'), fontsize=20)\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8') , fontsize=20)\n",
    "ax.set_title('Top 4 Línguas'.decode('utf-8'), fontsize=20, fontweight='bold')\n",
    "tweets_by_lang[:4].plot(ax=ax, kind='bar', color='mediumspringgreen')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_xlabel('Países'.decode('utf-8'), fontsize=20)\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8') , fontsize=20)\n",
    "ax.set_title('Top 5 Países'.decode('utf-8'), fontsize=20, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='lightskyblue')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "tweets['NaoVaiTerGolpe'] = tweets['text'].apply(lambda tweet: word_in_text('NaoVaiTerGolpe', tweet))\n",
    "tweets['TchauQuerida'] = tweets['text'].apply(lambda tweet: word_in_text('TchauQuerida', tweet))\n",
    "tweets['ForaDilma'] = tweets['text'].apply(lambda tweet: word_in_text('ForaDilma', tweet))\n",
    "# tweets['BrasilContraOGolpe'] = tweets['text'].apply(lambda tweet: word_in_text('BrasilContraOGolpe', tweet))\n",
    "# tweets['ForaCunha'] = tweets['text'].apply(lambda tweet: word_in_text('ForaCunha', tweet))\n",
    "\n",
    "# print tweets['FicaQuerida'].value_counts()[True]\n",
    "# print tweets['NaoVaiTerGolpe'].value_counts()[True]\n",
    "# print tweets['ForaPT'].value_counts()[True]\n",
    "\n",
    "# hashtags = ['ForaDilma', 'NaoVaiTerGolpe', 'TchauQuerida', 'BrasilContraOGolpe', 'ForaCunha']\n",
    "hashtags = ['ForaDilma', 'NaoVaiTerGolpe', 'TchauQuerida']\n",
    "tweets_by_hashtags = [tweets['ForaDilma'].value_counts()[True],\n",
    "                      tweets['NaoVaiTerGolpe'].value_counts()[True],\n",
    "                      tweets['TchauQuerida'].value_counts()[True]]\n",
    "#                       tweets['BrasilContraOGolpe'].value_counts()[True],\n",
    "#                       tweets['ForaCunha'].value_counts()[True]]\n",
    "\n",
    "plt.subplots(figsize=(9,9))\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.03, 0.03, 0.03)\n",
    "plt.pie(tweets_by_hashtags, explode=explode, labels=hashtags, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.legend(tweets_by_hashtags, loc=(1,.6))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['nao'] = tweets['text'].apply(lambda tweet: word_in_text('nao', tweet))\n",
    "tweets['sim'] = tweets['text'].apply(lambda tweet: word_in_text('sim', tweet))\n",
    "\n",
    "tweets['ImpeachmentDay'] = tweets['text'].apply(lambda tweet: word_in_text('sim', tweet) \n",
    "                                          or word_in_text('nao', tweet))\n",
    "\n",
    "# print tweets['nao'].value_counts()[True]\n",
    "# print tweets['sim'].value_counts()[True]\n",
    "# print tweets['ImpeachmentDay'].value_counts()[True]\n",
    "\n",
    "# print tweets[tweets['ImpeachmentDay'] == True]['ForaDilma'].value_counts()[True]\n",
    "# print tweets[tweets['ImpeachmentDay'] == True]['NaoVaiTerGolpe'].value_counts()[True]\n",
    "\n",
    "hashtags = ['ForaDilma', 'NaoVaiTerGolpe']\n",
    "tweets_by_hashtags = [tweets[tweets['ImpeachmentDay'] == True]['ForaDilma'].value_counts()[True], \n",
    "                      tweets[tweets['ImpeachmentDay'] == True]['NaoVaiTerGolpe'].value_counts()[True]]\n",
    "\n",
    "x_pos = list(range(len(hashtags)))\n",
    "width = 0.7\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.bar(x_pos, tweets_by_hashtags, width, alpha=1, color='sienna')\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8'), fontsize=20)\n",
    "ax.set_title('Ranking: ForaDilma vs. NaoVaiTerGolpe (Votação SIM x NÃO)'.decode('utf-8'),\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.set_xticks([p + 0.4 * width for p in x_pos])\n",
    "ax.set_xticklabels(hashtags)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))\n",
    "\n",
    "tweets_relevant = tweets[tweets['ImpeachmentDay'] == True]\n",
    "tweets_relevant_with_link = tweets_relevant[tweets_relevant['link'] != '']\n",
    "\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['TchauQuerida'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['ForaDilma'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['ForaCunha'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['NaoVaiTerGolpe'] == True]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets['moro'] = tweets['text'].apply(lambda tweet: word_in_text('moro', tweet))\n",
    "tweets['cunha'] = tweets['text'].apply(lambda tweet: word_in_text('cunha', tweet))\n",
    "tweets['bolsonaro'] = tweets['text'].apply(lambda tweet: word_in_text('bolsonaro', tweet))\n",
    "tweets['lula'] = tweets['text'].apply(lambda tweet: word_in_text('lula', tweet))\n",
    "tweets['temer'] = tweets['text'].apply(lambda tweet: word_in_text('temer', tweet))\n",
    "tweets['feliciano'] = tweets['text'].apply(lambda tweet: word_in_text('feliciano', tweet))\n",
    "\n",
    "hashtags = ['Sérgio Moro'.decode('utf-8'), 'Eduardo Cunha', 'Jair Bolsonaro', 'Lula', 'Michel Temer', 'Marcos Feliciano']\n",
    "tweets_by_hashtags = [tweets['moro'].value_counts()[True],\n",
    "                      tweets['cunha'].value_counts()[True],\n",
    "                      tweets['bolsonaro'].value_counts()[True],\n",
    "                      tweets['lula'].value_counts()[True],\n",
    "                      tweets['feliciano'].value_counts()[True],\n",
    "                      tweets['temer'].value_counts()[True]]\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'peachpuff', 'mediumturquoise']\n",
    "explode = (0.03, 0.03, 0.03, 0.05, 0.03, 0.03)\n",
    "plt.pie(tweets_by_hashtags, explode=explode, labels=hashtags, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.legend(tweets_by_hashtags, loc='best')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ainda ajustando aqui - Funções de busca de *tweets*, salvar e carregar json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "\n",
    "def twitter_search(q, max_results=1000, **kw):\n",
    "    search_results = twitter_api.search.tweets(q=q, count=1000, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    max_results = min(10000, max_results)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_resuts']\n",
    "        except KeyError, e:\n",
    "            break\n",
    "            \n",
    "        kwargs = dict([ kv.split('=')\n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = 'ForaInternetLimitada'\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "\n",
    "print json.dumps(results[0], indent=1)\n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_json(filename, data):\n",
    "    with io.open('data/{0}.json'.format(filename),\n",
    "                 'w', encoding='utf-8') as f:\n",
    "            f.write(unicode(json.dumps(data, ensure_ascii=False)))\n",
    "    \n",
    "def load_json(filename):\n",
    "    with io.open('data/{0}.json'.format(filename),\n",
    "                  encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "q = 'ForaInternetLimitada'\n",
    "\n",
    "\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "\n",
    "save_json(q, results)\n",
    "results = load_json(q)\n",
    "\n",
    "# print json.dumps(results, indent=1)\n",
    "\n",
    "# print json.dumps(br_trends, indent=1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
