{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mineração de *Tweets*\n",
    "Dados coletados durante o domingo (17/04/2015) de votação do Congresso para a continuação do processo de Impeachment da senhora Presidente Dilma Rousseff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import cartopy\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import operator \n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import random\n",
    "import re\n",
    "import seaborn as sns\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "from matplotlib import dates\n",
    "from matplotlib import rcParams\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpltools import style\n",
    "from nltk import FreqDist\n",
    "from nltk import bigrams \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from os import path\n",
    "from pandas.tseries.resample import TimeGrouper\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from scipy.misc import imread\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('mac_morpho')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"deep\", desat=.6)\n",
    "sns.set_context(rc={\"figure.figsize\": (8, 4)})\n",
    "style.use('ggplot')\n",
    "rcParams['axes.labelsize'] = 9\n",
    "rcParams['xtick.labelsize'] = 9\n",
    "rcParams['ytick.labelsize'] = 9\n",
    "rcParams['legend.fontsize'] = 7\n",
    "rcParams['font.serif'] = ['Computer Modern Roman']\n",
    "rcParams['text.usetex'] = False\n",
    "rcParams['figure.figsize'] = 20, 10\n",
    "\n",
    "# pd.set_option('display.max_colwidth', 200)\n",
    "# pd.options.display.mpl_style = 'default'\n",
    "# matplotlib.style.use('ggplot')\n",
    "# sns.set_context('talk')\n",
    "# sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_data_path = 'data-docker/tmp/dilmahash.txt'\n",
    "\n",
    "tweets_data = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        tweets_data.append(tweet)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print len(tweets_data)\n",
    "print tweets_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dados minerados anteriormente - OK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "tweets['text'] = map(lambda tweet: tweet['text'], tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "tweets['country'] = map(lambda tweet: tweet['place']['country']\n",
    "                        if tweet['place'] != None else None, tweets_data)\n",
    "\n",
    "tweets_by_lang = tweets['lang'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.tick_params(axis='x', labelsize=20)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_xlabel('Línguas'.decode('utf-8'), fontsize=20)\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8') , fontsize=20)\n",
    "ax.set_title('Top 4 Línguas'.decode('utf-8'), fontsize=20, fontweight='bold')\n",
    "tweets_by_lang[:4].plot(ax=ax, kind='bar', color='mediumspringgreen')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_by_country = tweets['country'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax.tick_params(axis='x', labelsize=15)\n",
    "ax.tick_params(axis='y', labelsize=15)\n",
    "ax.set_xlabel('Países'.decode('utf-8'), fontsize=20)\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8') , fontsize=20)\n",
    "ax.set_title('Top 5 Países'.decode('utf-8'), fontsize=20, fontweight='bold')\n",
    "tweets_by_country[:5].plot(ax=ax, kind='bar', color='lightskyblue')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "tweets['NaoVaiTerGolpe'] = tweets['text'].apply(lambda tweet: word_in_text('NaoVaiTerGolpe', tweet))\n",
    "tweets['TchauQuerida'] = tweets['text'].apply(lambda tweet: word_in_text('TchauQuerida', tweet))\n",
    "tweets['ForaDilma'] = tweets['text'].apply(lambda tweet: word_in_text('ForaDilma', tweet))\n",
    "# tweets['BrasilContraOGolpe'] = tweets['text'].apply(lambda tweet: word_in_text('BrasilContraOGolpe', tweet))\n",
    "# tweets['ForaCunha'] = tweets['text'].apply(lambda tweet: word_in_text('ForaCunha', tweet))\n",
    "\n",
    "# print tweets['FicaQuerida'].value_counts()[True]\n",
    "# print tweets['NaoVaiTerGolpe'].value_counts()[True]\n",
    "# print tweets['ForaPT'].value_counts()[True]\n",
    "\n",
    "# hashtags = ['ForaDilma', 'NaoVaiTerGolpe', 'TchauQuerida', 'BrasilContraOGolpe', 'ForaCunha']\n",
    "hashtags = ['ForaDilma', 'NaoVaiTerGolpe', 'TchauQuerida']\n",
    "tweets_by_hashtags = [tweets['ForaDilma'].value_counts()[True],\n",
    "                      tweets['NaoVaiTerGolpe'].value_counts()[True],\n",
    "                      tweets['TchauQuerida'].value_counts()[True]]\n",
    "#                       tweets['BrasilContraOGolpe'].value_counts()[True],\n",
    "#                       tweets['ForaCunha'].value_counts()[True]]\n",
    "\n",
    "plt.subplots(figsize=(9,9))\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral']\n",
    "explode = (0.03, 0.03, 0.03)\n",
    "plt.pie(tweets_by_hashtags, explode=explode, labels=hashtags, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.legend(tweets_by_hashtags, loc=(1,.6))\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['nao'] = tweets['text'].apply(lambda tweet: word_in_text('nao', tweet))\n",
    "tweets['sim'] = tweets['text'].apply(lambda tweet: word_in_text('sim', tweet))\n",
    "\n",
    "tweets['ImpeachmentDay'] = tweets['text'].apply(lambda tweet: word_in_text('sim', tweet) \n",
    "                                          or word_in_text('nao', tweet))\n",
    "\n",
    "# print tweets['nao'].value_counts()[True]\n",
    "# print tweets['sim'].value_counts()[True]\n",
    "# print tweets['ImpeachmentDay'].value_counts()[True]\n",
    "\n",
    "# print tweets[tweets['ImpeachmentDay'] == True]['ForaDilma'].value_counts()[True]\n",
    "# print tweets[tweets['ImpeachmentDay'] == True]['NaoVaiTerGolpe'].value_counts()[True]\n",
    "\n",
    "hashtags = ['ForaDilma', 'NaoVaiTerGolpe']\n",
    "tweets_by_hashtags = [tweets[tweets['ImpeachmentDay'] == True]['ForaDilma'].value_counts()[True], \n",
    "                      tweets[tweets['ImpeachmentDay'] == True]['NaoVaiTerGolpe'].value_counts()[True]]\n",
    "\n",
    "x_pos = list(range(len(hashtags)))\n",
    "width = 0.7\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plt.bar(x_pos, tweets_by_hashtags, width, alpha=1, color='sienna')\n",
    "ax.set_ylabel('Número de tweets'.decode('utf-8'), fontsize=20)\n",
    "ax.set_title('Ranking: ForaDilma vs. NaoVaiTerGolpe (Votação SIM x NÃO)'.decode('utf-8'),\n",
    "             fontsize=15, fontweight='bold')\n",
    "ax.set_xticks([p + 0.4 * width for p in x_pos])\n",
    "ax.set_xticklabels(hashtags)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_link(text):\n",
    "    regex = r'https?://[^\\s<>\"]+|www\\.[^\\s<>\"]+'\n",
    "    match = re.search(regex, text)\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return ''\n",
    "\n",
    "tweets['link'] = tweets['text'].apply(lambda tweet: extract_link(tweet))\n",
    "\n",
    "tweets_relevant = tweets[tweets['ImpeachmentDay'] == True]\n",
    "tweets_relevant_with_link = tweets_relevant[tweets_relevant['link'] != '']\n",
    "\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['TchauQuerida'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['ForaDilma'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['ForaCunha'] == True]['link']\n",
    "print tweets_relevant_with_link[tweets_relevant_with_link['NaoVaiTerGolpe'] == True]['link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['moro'] = tweets['text'].apply(lambda tweet: word_in_text('moro', tweet))\n",
    "tweets['cunha'] = tweets['text'].apply(lambda tweet: word_in_text('cunha', tweet))\n",
    "tweets['bolsonaro'] = tweets['text'].apply(lambda tweet: word_in_text('bolsonaro', tweet))\n",
    "tweets['lula'] = tweets['text'].apply(lambda tweet: word_in_text('lula', tweet))\n",
    "tweets['temer'] = tweets['text'].apply(lambda tweet: word_in_text('temer', tweet))\n",
    "tweets['feliciano'] = tweets['text'].apply(lambda tweet: word_in_text('feliciano', tweet))\n",
    "\n",
    "hashtags = ['Sérgio Moro'.decode('utf-8'), 'Eduardo Cunha', 'Jair Bolsonaro', 'Lula', 'Michel Temer', 'Marcos Feliciano']\n",
    "tweets_by_hashtags = [tweets['moro'].value_counts()[True],\n",
    "                      tweets['cunha'].value_counts()[True],\n",
    "                      tweets['bolsonaro'].value_counts()[True],\n",
    "                      tweets['lula'].value_counts()[True],\n",
    "                      tweets['feliciano'].value_counts()[True],\n",
    "                      tweets['temer'].value_counts()[True]]\n",
    "\n",
    "plt.subplots(figsize=(8,8))\n",
    "colors = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'peachpuff', 'mediumturquoise']\n",
    "explode = (0.03, 0.03, 0.03, 0.05, 0.03, 0.03)\n",
    "plt.pie(tweets_by_hashtags, explode=explode, labels=hashtags, colors=colors,\n",
    "        autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.legend(tweets_by_hashtags, loc='best')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes de *token* e plots com *vicent*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = \"RT @medeirosthiiago: testando exemplo TCC! :D http://example.com #ImpeachmentDay\"\n",
    "print preprocess(tweet)\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        count_all.update(terms_all)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "punctuation = list(string.punctuation)\n",
    "# stop = stopwords.words('english') + punctuation + ['RT', 'o']\n",
    "stop = nltk.corpus.stopwords.words('portuguese') + punctuation + ['RT', 'n']\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]\n",
    "        count_all.update(terms_stop)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_single = set(terms_all)\n",
    "        terms_hash = [term for term in preprocess(tweet['text']) \n",
    "                      if term.startswith('#')]\n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                      if term not in stop and\n",
    "                      not term.startswith(('#', '@'))] \n",
    "        count_all.update(terms_hash)\n",
    "    except:\n",
    "        continue        \n",
    "\n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_bigram = bigrams(terms_stop)\n",
    "        count_all.update(terms_bigram)\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "\n",
    "print count_all.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "com = defaultdict(lambda : defaultdict(int))\n",
    "\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "count_all = Counter()\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                      if term not in stop \n",
    "                      and not term.startswith(('#', '@'))]\n",
    "        for i in range(len(terms_only)-1):            \n",
    "            for j in range(i+1, len(terms_only)):\n",
    "                w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "                if w1 != w2:\n",
    "                    com[w1][w2] += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "com_max = []\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])\n",
    "\n",
    "search_word = sys.argv[1]\n",
    "count_search = Counter()\n",
    "print 'ok'\n",
    "\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    "        if search_word in terms_only:\n",
    "            count_search.update(terms_only)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "            \n",
    "print \"Co-occurrence for %s:\" % (search_word)\n",
    "print count_search.most_common(20)\n",
    "\n",
    "\n",
    "\n",
    "import vincent\n",
    " \n",
    "word_freq = count_terms_only.most_common(20)\n",
    "labels, freq = zip(*word_freq)\n",
    "data = {'data': freq, 'x': labels}\n",
    "bar = vincent.Bar(data, iter_idx='x')\n",
    "bar.to_json('term_freq.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date = []\n",
    "tweets_file = open(tweets_data_path, \"r\")\n",
    "for line in tweets_file:\n",
    "    try:\n",
    "        tweet = json.loads(line)        \n",
    "        terms_hash = [term for term in preprocess(tweet['text'])]\n",
    "        # track when the hashtag is mentioned\n",
    "        if '#ImpeachmentDay' in terms_hash:\n",
    "            date.append(tweet['created_at'])\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "ones = [1]*len(date)\n",
    "idx = pandas.DatetimeIndex(date)\n",
    "date_tweet = pandas.Series(ones, index=idx)\n",
    " \n",
    "\n",
    "per_minute = date_tweet.resample('1H', how='count')\n",
    "\n",
    "vincent.core.initialize_notebook()\n",
    "\n",
    "time_chart = vincent.Line(date_tweet)\n",
    "time_chart.axis_titles(x='Tempo', y='Frequencia')\n",
    "time_chart.colors(brew='Spectral')\n",
    "time_chart.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match_data = dict(ForaDilma=per_minute_i, NaoVaiTerGolpe=per_minute_s, ForaCunha=per_minute_e)\n",
    "\n",
    "\n",
    "all_matches = pandas.DataFrame(data=match_data,\n",
    "                               index=per_minute_i.index)\n",
    "\n",
    "all_matches = all_matches.resample('30S', how='sum').fillna(0)\n",
    " \n",
    "\n",
    "vincent.core.initialize_notebook()\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart = vincent.Line(date_tweet)\n",
    "time_chart.axis_titles(x='Tempo', y='Frequencia')\n",
    "time_chart.colors(brew='Spectral')\n",
    "time_chart.legend(title='Matches')\n",
    "time_chart.display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Testes com a ferramenta *arrow* e horários"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = pd.DataFrame()\n",
    "\n",
    "tweets['created_at'] = map(lambda tweet: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')), tweets_data)\n",
    "tweets['user'] = map(lambda tweet: tweet['user']['screen_name'], tweets_data)\n",
    "tweets['user_followers_count'] = map(lambda tweet: tweet['user']['followers_count'], tweets_data)\n",
    "tweets['retweet_count'] = map(lambda tweet: tweet['retweet_count'], tweets_data)\n",
    "tweets['favorite_count'] = map(lambda tweet: tweet['favorite_count'], tweets_data)\n",
    "\n",
    "tweets['text'] = map(lambda tweet: tweet['text'].encode('utf-8'), tweets_data)\n",
    "tweets['lang'] = map(lambda tweet: tweet['lang'], tweets_data)\n",
    "tweets['Location'] = map(lambda tweet: tweet['place']['country'] if tweet['place'] != None else None, tweets_data)\n",
    "\n",
    "tweets.info()\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_time = pd.Series()\n",
    "\n",
    "tweets_time['created_at'] = map(lambda tweet: time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')), tweets_data)\n",
    "tweets_time['created_at'] = pd.to_datetime(pd.Series(tweets_time['created_at']))\n",
    "tweets_time.se('created_at', drop=False, inplace=True)\n",
    "tweets_time.index = tweets_time.index.tzlocalize('GMT').tzconvert('EST')\n",
    "tweets_time.index = tweets_time.index - DateOffset(hours = 12)\n",
    "tweets_time.index\n",
    "\n",
    "tweets_time.head()\n",
    "tweets_time.mean()\n",
    "print 'testou'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tz = pytz.timezone('Europe/Warsaw')\n",
    "tweets.set_index('created_at', drop=False, inplace=True)\n",
    "tweets.index = tweets.index\n",
    "tweets.index = tweets.tz_localize('GMT').tz_convert('EST')\n",
    "tweets.index = tweets.index - DateOffset(hours = 12)\n",
    "\n",
    "tweets_time = tweets['created_at']\n",
    "tweets.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets['created_at'].value_counts())\n",
    "df['hour'] = df.index\n",
    "\n",
    "hours = [item.split(\" \")[0] for item in df['hour'].values]\n",
    "df['hours'] = hours\n",
    "grouped_tweets = df[['hours']].groupby('hours')\n",
    "tweet_growth = grouped_tweets.sum()\n",
    "tweet_growth['hours']= tweet_growth.index\n",
    "\n",
    "tweet_growth\n",
    "# df.info()\n",
    "# df.tail()\n",
    "# df.mean()\n",
    "\n",
    "df.describe()\n",
    "\n",
    "import vincent\n",
    "vincent.core.initialize_notebook()\n",
    "area = vincent.Area(df)\n",
    "area.colors(brew='Spectral')\n",
    "area.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print stopwordsnltk.corpus.mac_morpho.words()\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "text = tweets['text']\n",
    "\n",
    "\n",
    "tokens = []\n",
    "for txt in text.values:\n",
    "    tokens.extend([t.lower().strip(\":,.\") for t in txt.split()])\n",
    "\n",
    "\n",
    "\n",
    "filtered_tokens = [w for w in tokens if not w in stop]\n",
    "\n",
    "freqdist = nltk.FreqDist(filtered_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist.keys()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "freq_dist.plot(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flyers.source.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando outras formas de plots e *wordcloud*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_of_original_tweets = [element for element in tweets['text'].values if not element.startswith('RT')]\n",
    "print list_of_original_tweets[0]\n",
    "\n",
    "print \"Numero de Tweets originais : \" + str(len(list_of_original_tweets))\n",
    "\n",
    "list_of_retweets = [element for element in tweets['text'].values if element.startswith('RT')]\n",
    "print \"Numero de Retweets : \" + str(len(list_of_retweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_tweets_per_category(category, title, x_title, y_title, top_n=5, output_filename=\"plot.png\"):\n",
    "    tweets_by_cat = category.value_counts()\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.tick_params(axis='x')\n",
    "    ax.tick_params(axis='y')\n",
    "    ax.set_xlabel(x_title)\n",
    "    ax.set_ylabel(y_title)\n",
    "    ax.set_title(title)\n",
    "    tweets_by_cat[:top_n].plot(ax=ax, kind='bar')\n",
    "    fig.savefig(output_filename)\n",
    "    fig.show()\n",
    "\n",
    "plot_tweets_per_category(tweets['lang'], \"#ImpeachmentDay por Idioma\", \n",
    "                         \"Idioma\", \n",
    "                         \"Numero de Tweets\", \n",
    "                         2000,\n",
    "                         \"ImpeachmentDay_per_language.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_tweets_per_category(tweets['Location'], \n",
    "                             \"#ImpeachmentDay por Localizacao\", \n",
    "                             \"Localizacao\", \n",
    "                             \"Numero de Tweets\", 2000,\n",
    "                             \"ImpeachmentDay_per_location.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_tweets_per_category(tweets['user'], \n",
    "                             \"#ImpeachmentDay usuarios ativos\", \n",
    "                             \"Usuarios\", \n",
    "                             \"Numero de Tweets\", 20,\n",
    "                             \"ImpeachmentDay_users.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_distribution(category, title, x_title, y_title, output_filename=\"plot.png\"):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.tick_params(axis='x')\n",
    "        ax.tick_params(axis='y')\n",
    "        ax.set_xlabel(x_title)\n",
    "        ax.set_ylabel(y_title)\n",
    "        ax.set_title(title)\n",
    "        sns.distplot(category.values, rug=True, hist=True);\n",
    "        fig.savefig(output_filename)\n",
    "\n",
    "\n",
    "plot_distribution(tweets['retweet_count'], \n",
    "                      \"#ImpeachmentDay distribuicao de Retweets\", \"\", \"\",\n",
    "                      \"retweets_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets['created_at'].value_counts(), columns=['number_tweets'])\n",
    "df['date'] = df.index\n",
    "df.head()\n",
    "\n",
    "days = [item.split(\" \")[0] for item in df['date'].values]\n",
    "df['days'] = days\n",
    "grouped_tweets = df[['days', 'number_tweets']].groupby('days')\n",
    "tweet_growth = grouped_tweets.sum()\n",
    "tweet_growth['days']= tweet_growth.index\n",
    "\n",
    "tweet_growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "x_pos = np.arange(len(tweet_growth['days'].values))\n",
    "ax.bar(x_pos, tweet_growth['number_tweets'].values, align='center')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_title('#ImpeachmentDay hashtag growth')\n",
    "ax.set_ylabel(\"number tweets\")\n",
    "ax.set_xticklabels(tweet_growth['days'].values)\n",
    "fig.savefig('ImpeachmentDay.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join(tweets['text'].values.astype(str))\n",
    "\n",
    "no_urls_no_tags = \" \".join([word for word in text.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=2000,\n",
    "                      stopwords=STOPWORDS, width=1800, height=1400).generate(no_urls_no_tags)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join(tweets['text'].values.astype(str))\n",
    "\n",
    "no_urls_no_tags = \" \".join([word for word in text.split()\n",
    "                            if 'http' not in word\n",
    "                                and not word.startswith('@')\n",
    "                                and word != 'RT'\n",
    "                            ])\n",
    "\n",
    "tweet_coloring = imread(path.join(\"dilma2.png\"))\n",
    "\n",
    "wc = WordCloud(background_color=\"white\", max_words=2000, mask=tweet_coloring,\n",
    "               stopwords=STOPWORDS, max_font_size=40, random_state=42)\n",
    "\n",
    "wc.generate(no_urls_no_tags)\n",
    "\n",
    "image_colors = ImageColorGenerator(tweet_coloring)\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.savefig('mozsprint.png', dpi=300)\n",
    "plt.imshow(wc.recolor(color_func=image_colors))\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(tweet_coloring, cmap=plt.cm.gray)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* * *\n",
    "* * *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de busca de *tweets*, salvar e carregar json (não utilizado ainda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, json\n",
    "\n",
    "def twitter_search(q, max_results=1000, **kw):\n",
    "    search_results = twitter_api.search.tweets(q=q, count=1000, **kw)\n",
    "    \n",
    "    statuses = search_results['statuses']\n",
    "    \n",
    "    max_results = min(10000, max_results)\n",
    "    \n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            next_results = search_results['search_metadata']['next_resuts']\n",
    "        except KeyError, e:\n",
    "            break\n",
    "            \n",
    "        kwargs = dict([ kv.split('=')\n",
    "                        for kv in next_results[1:].split(\"&\") ])\n",
    "        \n",
    "        search_results = twitter_api.search.tweets(**kwargs)\n",
    "        statuses += search_results['statuses']\n",
    "        \n",
    "        if len(statuses) > max_results:\n",
    "            break\n",
    "            \n",
    "    return statuses\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "\n",
    "q = 'ImpeachmentDay'\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "\n",
    "print json.dumps(results[0], indent=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_json(filename, data):\n",
    "    with io.open('data/{0}.json'.format(filename),\n",
    "                 'w', encoding='utf-8') as f:\n",
    "            f.write(unicode(json.dumps(data, ensure_ascii=False)))\n",
    "    \n",
    "def load_json(filename):\n",
    "    with io.open('data/{0}.json'.format(filename),\n",
    "                  encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "\n",
    "q = 'ImpeachmentDay'\n",
    "\n",
    "twitter_api = oauth_login()\n",
    "results = twitter_search(twitter_api, q, max_results=1000)\n",
    "\n",
    "save_json(q, results)\n",
    "results = load_json(q)\n",
    "\n",
    "# print json.dumps(results, indent=1)\n",
    "# print json.dumps(br_trends, indent=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
